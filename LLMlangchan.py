# -*- coding: utf-8 -*-
"""
Hybrid RAG (SQL + pgvector) â€” Schema-specific single file (ì£¼ì œ ê¸°ë°˜ QID ë§¤í•‘ ì ìš© ë²„ì „)

ì¶”ê°€ëœ ê¸°ëŠ¥:
1) semantic_queryì—ì„œ "ë¶„ì„ ì£¼ì œ" ìë™ ì¶”ì¶œ (extract_topic)
2) ë¶„ì„ ì£¼ì œ â†’ QID ë§¤í•‘ í…Œì´ë¸” TOPIC_TO_QIDS ì ìš©
3) 3ë‹¨ê³„: QID ë§¤í•‘ ìš°ì„  ì ìš© â†’ ì—†ìœ¼ë©´ ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ fallback
4) 4ë‹¨ê³„: ê¸°ì¡´ hybrid ê²€ìƒ‰ì€ ìœ ì§€ (QID í•„í„°ê°€ ìë™ ë°˜ì˜ë¨)
"""

import os, json, re
from typing import List, Dict, Any, Tuple

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv(), override=True)

import psycopg2
from psycopg2.extras import RealDictCursor

from openai import OpenAI
from sentence_transformers import SentenceTransformer
import numpy as np

# -----------------------
# ENV
# -----------------------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY missing")
oai = OpenAI(api_key=OPENAI_API_KEY)

DB = dict(
    host=os.getenv("DB_HOST"),
    port=int(os.getenv("DB_PORT", "5432")),
    dbname=os.getenv("DB_NAME"),
    user=os.getenv("DB_USER"),
    password=os.getenv("DB_PASSWORD"),
)

# ì„ë² ë”© ëª¨ë¸ (KURE)
EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "nlpai-lab/KURE-v1")
_device = "cuda" if os.getenv("USE_CUDA", "0") == "1" else "cpu"
_embedder = SentenceTransformer(EMB_MODEL_NAME, device=_device)

PGVECTOR_OP = "<=>"   # cosine

# -----------------------
# 1) ìì—°ì–´ â†’ filters + semantic_query
# -----------------------
SYSTEM_PROMPT = """
ë„ˆëŠ” PostgreSQL ê¸°ë°˜ ì§ˆì˜ ë¶„ì„ê¸°ë‹¤.
ì•„ë˜ ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë‘ ì¡°ê°ìœ¼ë¡œ ë¶„í•´í•´ JSONë§Œ ë°˜í™˜í•˜ë¼:
- filters: [{column, operator, value}]  (metadata í…Œì´ë¸” ì»¬ëŸ¼ë§Œ ì‚¬ìš©)
- semantic_query: string

ì»¬ëŸ¼: gender('ë‚¨ì„±'/'ì—¬ì„±'), age(INT), birth_year(INT),
      region(VARCHAR), mobile_carrier('SKT','KT','LGU+','Wiz')

í•„í„° ê·œì¹™:
- "30ëŒ€" â†’ age >= 30 AND age < 40
- "1990ë…„ëŒ€ìƒ" â†’ birth_year >= 1990 AND birth_year < 2000
- "ì„œìš¸" â†’ region LIKE 'ì„œìš¸%'
ìŠ¤í‚¤ë§ˆ ë°–(ì§ì—…, ì·¨í–¥ ë“±)ì€ semantic_queryì—ë§Œ ë‚¨ê²¨ë¼.

ë°˜ë“œì‹œ {"filters":[...], "semantic_query": "..."} í˜•ì‹ë§Œ ì¶œë ¥.
"""

SCHEMA = {
    "type": "object",
    "properties": {
        "filters": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "column": {"type": "string"},
                    "operator": {"type": "string"},
                    "value": {"type": "string"},
                },
                "required": ["column", "operator", "value"]
            }
        },
        "semantic_query": {"type": "string"}
    },
    "required": ["filters", "semantic_query"]
}

def parse_query(user_query: str) -> dict:
    resp = oai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_query},
        ],
        tools=[{
            "type": "function",
            "function": {
                "name": "extract",
                "description": "ì‚¬ìš©ì ìš”ì²­ì„ filters/semantic_queryë¡œ ë¶„í•´",
                "parameters": SCHEMA
            }
        }],
        tool_choice={"type": "function", "function": {"name": "extract"}},
        temperature=0.0
    )
    try:
        tool = resp.choices[0].message.tool_calls[0]
        return json.loads(tool.function.arguments)
    except:
        return {"filters": [], "semantic_query": user_query}

# -----------------------
# 2) filters â†’ metadata WHERE
# -----------------------
ALLOWED_COLS = {"gender","age","birth_year","region","mobile_carrier"}
ALLOWED_OPS  = {"=","!=","LIKE",">",">=","<","<="}

def build_where(filters: List[Dict[str,str]]) -> Tuple[str, list]:
    if not filters: return "", []
    conds, params = [], []
    for f in filters:
        c, op, v = f["column"], f["operator"], f["value"]
        if c not in ALLOWED_COLS or op not in ALLOWED_OPS:
            continue
        conds.append(f"{c} {op} %s")
        params.append(v)
    return (" WHERE " + " AND ".join(conds), params) if conds else ("", [])

# -----------------------
# DB util
# -----------------------
def db_conn():
    return psycopg2.connect(**DB, cursor_factory=RealDictCursor)

# -----------------------
# Vector encoder
# -----------------------
def embed(text: str) -> np.ndarray:
    if not text:
        text = "general preference"
    v = _embedder.encode([text], normalize_embeddings=True)[0]
    return v.astype(np.float32)

# -----------------------
# (ì¶”ê°€) ë¶„ì„ ì£¼ì œ ì¶”ì¶œ í•¨ìˆ˜
# -----------------------
def extract_topic(semantic_query: str) -> str:
    prompt = f"""
        ì•„ë˜ ë¬¸ì¥ì—ì„œ 'ë¶„ì„í•  ì£¼ì œ(what to analyze)'ë§Œ í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ë¡œ ì¶”ì¶œí•˜ë¼.

        ì˜ˆ:
        - 30ëŒ€ ë‚¨ì„±ì˜ ì†Œë¹„ íŒ¨í„´ â†’ 'ì†Œë¹„'
        - ì„œìš¸ 20ëŒ€ì˜ ì·¨ì—… í˜„í™© â†’ 'ì·¨ì—…'
        - SKT ì‚¬ìš©ìë“¤ì˜ ë¶ˆë§Œ ìš”ì¸ â†’ 'ë¶ˆë§Œ ìš”ì¸' 

    ë¬¸ì¥: "{semantic_query}"
    ì£¼ì œ:
    """
    try:
        res = oai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return res.choices[0].message.content.strip()
    except:
        return semantic_query

# -----------------------
# (ì¶”ê°€) ì£¼ì œ â†’ QIDs ë§¤í•‘ í…Œì´ë¸”
# -----------------------
TOPIC_TO_QIDS = {
    "ì†Œë¹„": ["Q2", "Q3"],
    "ì†Œë¹„ íŒ¨í„´": ["Q2", "Q3"],
    "ì¹´í…Œê³ ë¦¬ ë¹„ì¤‘": ["Q3", "Q5"],
    "ê´€ì‹¬ ë¶„ì•¼": ["Q7", "Q8", "Q9_1"],
    "ê´€ì‹¬ì‚¬": ["Q7", "Q8", "Q9_1"],
    "ì´ìš©ë¥ ": ["Q5", "Q6"],
    "ë¶ˆë§Œ": ["Q12", "Q13"],
    "ë§Œì¡±ë„": ["Q10", "Q11"]
}

# -----------------------
# Hybrid RAG Pipeline
# -----------------------
def hybrid_answer(user_query: str,
                   k_questions: int = 5,
                   k_answers: int = 500,
                   topn_return: int = 30) -> Dict[str,Any]:

    print(f"\n===== [RAG ì‹œì‘] ì§ˆë¬¸: \"{user_query}\" =====")

    # ---------------- 1ë‹¨ê³„: LLM íŒŒì‹± ----------------
    parsed = parse_query(user_query)
    filters = parsed.get("filters", [])
    semantic_query = parsed.get("semantic_query", "").strip()

    # ---------------- 2ë‹¨ê³„: metadata filter â†’ mb_sn ----------------
    where_sql, params = build_where(filters)
    with db_conn() as conn, conn.cursor() as cur:
        cur.execute(f"SELECT mb_sn FROM metadata{where_sql};", params)
        mb_list = [r["mb_sn"] for r in cur.fetchall()]
    mb_set = set(mb_list)

    # ================================================================
    # ğŸ”¥ 3ë‹¨ê³„: ë¶„ì„ ì£¼ì œ ê¸°ë°˜ QID ë§¤í•‘ ì ìš© (ì—¬ê¸°ê°€ ìƒˆë¡œ êµì²´ëœ ë¶€ë¶„)
    # ================================================================
    print("\n[ 3ë‹¨ê³„: ê´€ë ¨ ì§ˆë¬¸(QID) ì„ íƒ ]")

    # 3-A) ì£¼ì œ ì¶”ì¶œ
    topic = extract_topic(semantic_query)
    print(f"  - ë¶„ì„ ì£¼ì œ: {topic}")

    # 3-B) ë§¤í•‘ ìš°ì„  ì ìš©
    mapped_qids = TOPIC_TO_QIDS.get(topic, [])

    if mapped_qids:
        print(f"  - ì£¼ì œ ê¸°ë°˜ ë§¤í•‘ëœ QIDs ì‚¬ìš©: {mapped_qids}")
        qids = mapped_qids
    else:
        # fallback: ê¸°ì¡´ q_vector ê¸°ë°˜ ê²€ìƒ‰
        print("  - ë§¤í•‘ëœ ì£¼ì œê°€ ì—†ìŒ â†’ ë²¡í„° ê¸°ë°˜ ì§ˆë¬¸ ê²€ìƒ‰ ì‹¤í–‰.")
        q_vec = embed(semantic_query)

        with db_conn() as conn, conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT codebook_id
                FROM codebooks
                ORDER BY q_vector {PGVECTOR_OP} %s::vector
                LIMIT %s;
                """,
                (q_vec.tolist(), k_questions)
            )
            qids = [r["codebook_id"] for r in cur.fetchall()]

    print(f"  - ìµœì¢… ì„ íƒëœ QID ëª©ë¡: {qids}")

    if not qids:
        return {
            "answer": "í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "filters": filters,
            "semantic_query": semantic_query,
            "question_ids": [],
            "samples": []
        }

    # =================================================================
    # ğŸ”¥ 4ë‹¨ê³„: answers êµì°¨ í•„í„°ë§ (QID + mb_sn + vector)
    #     â€» ê¸°ì¡´ SQL ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ QIDsê°€ ìë™ ë°˜ì˜ë¨.
    # =================================================================
    print("\n[ 4ë‹¨ê³„: answers êµì°¨ í•„í„°ë§ ]")
    print(f"  - í•„í„°ë§ ëŒ€ìƒ ì‘ë‹µì ìˆ˜: {len(mb_set)}ëª…")

    q_vec = embed(semantic_query)

    sql_select = f"""
        SELECT a.answer_id, a.mb_sn, a.question_id, a.answer_value,
               a.a_vector {PGVECTOR_OP} %s::vector AS distance,
               c.codebook_data
        FROM answers a
        LEFT JOIN codebooks c ON a.question_id = c.codebook_id
    """
    sql_order = f"""
        ORDER BY a.a_vector {PGVECTOR_OP} %s::vector
        LIMIT %s
    """

    with db_conn() as conn, conn.cursor() as cur:
        rows = []

        if mb_set:
            sql1 = f"""
                {sql_select}
                WHERE a.question_id = ANY(%s) AND a.mb_sn = ANY(%s)
                {sql_order};
            """
            cur.execute(sql1,
                        (q_vec.tolist(), qids, list(mb_set),
                         q_vec.tolist(), k_answers))
            rows = cur.fetchall()

        if not rows:
            sql2 = f"""
                {sql_select}
                WHERE a.question_id = ANY(%s)
                {sql_order};
            """
            cur.execute(sql2,
                        (q_vec.tolist(), qids,
                         q_vec.tolist(), k_answers))
            rows = cur.fetchall()

    if not rows:
        return {
            "answer": "ì¡°ê±´ì— ë§ëŠ” ì‘ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "filters": filters,
            "semantic_query": semantic_query,
            "question_ids": qids,
            "samples": []
        }

    # ================================================================
    # ì´í›„ ê³¼ì •(ì •ê·œí™”, ìš”ì•½, í†µê³„)ì€ ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€
    # ================================================================
    unique_resp = set(r['mb_sn'] for r in rows)

    # (ì´í•˜ ì›ë˜ ë„ˆì˜ ì½”ë“œ ê·¸ëŒ€ë¡œ)
    # ---------------------------------------------------------------
    # ê°ê´€ì‹ ë³´ê¸°ë¥¼ ë¼ë²¨ë¡œ ë³€í™˜
    def _build_choice_map(codebook_data: dict) -> dict:
        m = {}
        if not codebook_data:
            return m
        items = codebook_data.get("answers") or []
        for it in items:
            if not isinstance(it, dict): continue
            key = str(it.get("qi_val") or it.get("q_val") or it.get("value") or "").strip()
            val = (it.get("qi_title") or it.get("label") or it.get("text") or it.get("name") or "").strip()
            if key and val: m[key] = val
        return m

    def _translate(raw_value, cmap):
        if raw_value is None: return ""
        parts = [p for p in re.split(r"[,\s]+", str(raw_value).strip()) if p]
        out = []
        used = set()
        for p in parts:
            label = cmap.get(p, p)
            if label not in used:
                used.add(label)
                out.append(label)
        return ", ".join(out)

    for r in rows:
        cmap = _build_choice_map(r.get("codebook_data"))
        if cmap:
            r["answer_value_text"] = _translate(r.get("answer_value"), cmap)
        else:
            r["answer_value_text"] = r.get("answer_value")

    # --------------------- í†µê³„ ê³„ì‚° ---------------------
    from collections import Counter
    answer_texts = [r["answer_value_text"] for r in rows]
    counter = Counter(answer_texts)
    top_items = counter.most_common(10)

    stats_text = "\n".join(
        [f"  â€¢ {v}: {c}ëª… ({c/len(rows)*100:.1f}%)" for v,c in top_items]
    )

    final_text = "\n".join(f"- {t}" for t in answer_texts)

    summary_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì¡°ê±´ê³¼ ë‹µë³€ ë¶„í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì  ìˆ˜ì¹˜ê°€ í¬í•¨ëœ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

[í•„í„°]
{json.dumps(filters, ensure_ascii=False)}

[ì£¼ì œ]
{semantic_query}

[QID ëª©ë¡]
{qids}

[í†µê³„]
{stats_text}

[ì‘ë‹µ ìƒ˜í”Œ]
{final_text}
"""

    try:
        summary = oai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.2
        )
        answer = summary.choices[0].message.content
    except:
        answer = "ìš”ì•½ ìƒì„± ì˜¤ë¥˜ ë°œìƒ"

    return {
        "answer": answer,
        "filters": filters,
        "semantic_query": semantic_query,
        "question_ids": qids,
        "samples": rows[:topn_return],
        "statistics": {
            "total_respondents": len(unique_resp),
            "total_answers": len(rows)
        }
    }

# quick test
if __name__ == "__main__":
    q = "ì„œìš¸ ì‚¬ëŠ” 30ëŒ€ ë‚¨ì„± ì†Œë¹„ íŒ¨í„´ ì•Œë ¤ì¤˜"
    res = hybrid_answer(q)
    print(json.dumps(res, ensure_ascii=False, indent=2))
