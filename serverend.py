# -*- coding: utf-8 -*-
import os, re, json
from uuid import uuid4
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Any, Tuple
from datetime import date
import copy 

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
from flask import Flask, request, jsonify, session 
from flask_cors import CORS
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer

# =================================================================
# 0) í™˜ê²½ ë° ì „ì—­ ì„¤ì •
# =================================================================

BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR / ".env"
load_dotenv(ENV_PATH)

# --- DB/API ì„¤ì • (ê¸°ì¡´ ì„œë²„ íŒŒì¼ ë‚´ìš© ìœ ì§€) ---
DB = dict(
    host=os.getenv("DB_HOST", "localhost"),
    port=int(os.getenv("DB_PORT", "5432")),
    dbname=os.getenv("DB_NAME", "mydb"),
    user=os.getenv("DB_USER", "postgres"),
    password=os.getenv("DB_PASSWORD", "7302"),
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_MODEL     = os.getenv("CHAT_MODEL", "gpt-4o-mini")
EMBED_MODEL    = os.getenv("EMBED_MODEL", "nlpai-lab/KURE-v1") 

OOS_WORDS = ["ë‚ ì”¨","ì£¼ê°€","í™˜ìœ¨","ë‰´ìŠ¤","êµí†µ","ì‹œê°„","ì£¼ì†Œ","íƒë°°"]

app = Flask(__name__)
CORS(app)
# ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ë¥¼ ìœ„í•´ Flask Secret Key ì„¤ì • (í•„ìˆ˜)
app.secret_key = os.urandom(24) 

# --- ëŒ€í™” ê¸°ë¡ (History) ì €ì¥ì†Œ (ì„ì‹œ) ---
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Redis, DB ë“±ìœ¼ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤.
CONVERSATION_HISTORY: Dict[str, List[Dict[str, str]]] = {} 

# =================================================================
# 1) DB ì—°ê²° ë° ì¿¼ë¦¬ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# =================================================================

def get_conn():
    # ... (ê¸°ì¡´ get_conn í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    try:
        return psycopg2.connect(**DB)
    except Exception as e:
        print(f"[DB] connection failed: {e}")
        return None

def fetch_rows(limit: int | None = None) -> List[Dict[str, Any]]:
    # ... (ê¸°ì¡´ fetch_rows í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    conn = get_conn()
    if not conn: return []
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            sql = 'SELECT mb_sn, "Q10", "Q11", "Q12_1", "Q12_2", profile_vector::text AS profile_vector FROM respondents'
            if limit:
                sql += " LIMIT %s"
                cur.execute(sql, (limit,))
            else:
                cur.execute(sql)
            return cur.fetchall()
    finally: conn.close()

def _make_where_and_params(filters: Dict[str, Any]) -> tuple[str, list]:
    # ... (ê¸°ì¡´ _make_where_and_params í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    where, params = [], []

    g = filters.get("gender")
    if g:
        if g in ("ë‚¨","ì—¬"): g = "M" if g == "ë‚¨" else "F"
        where.append('"Q10" = %s')
        params.append(g)

    dec = filters.get("decade")
    if dec is not None:
        lo, hi = int(dec), int(dec)+9
        where.append('(EXTRACT(YEAR FROM CURRENT_DATE)::int - "Q11"::int BETWEEN %s AND %s)')
        params += [lo, hi]

    if filters.get("sido"):
        where.append('"Q12_1" = %s')
        params.append(filters["sido"])

    if filters.get("sigungu"):
        where.append('"Q12_2" = %s')
        params.append(filters["sigungu"])

    return (("WHERE " + " AND ".join(where)) if where else ""), params

# =================================================================
# 2) ì„ë² ë”© ë° ìœ í‹¸ë¦¬í‹° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# =================================================================

@lru_cache(maxsize=1)
def _load_embedder():
    # ... (ê¸°ì¡´ _load_embedder í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    print(f"[EMBED] loading: {EMBED_MODEL}")
    try:
        return SentenceTransformer(EMBED_MODEL)
    except Exception as e:
        print(f"[EMBED] primary load failed: {e}")
        fb = "jhgan/ko-sroberta-multitask"
        print(f"[EMBED] fallback -> {fb}")
        return SentenceTransformer(fb)

@lru_cache(maxsize=4096)
def embed_text(text: str) -> np.ndarray:
    # ... (ê¸°ì¡´ embed_text í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    model = _load_embedder()
    v = np.array(model.encode(text, normalize_embeddings=True), dtype=np.float32)
    n = np.linalg.norm(v)
    return (v / n) if n else v

def cos_sim(u: np.ndarray, v: np.ndarray) -> float:
    # ... (ê¸°ì¡´ cos_sim í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    return float(np.dot(u, v))

def fetch_topk_by_cosine(query: str, k: int = 5, prelimit: int = 800) -> List[Dict[str, Any]]:
    # ... (ê¸°ì¡´ fetch_topk_by_cosine í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    qv = embed_text(query)
    rows = fetch_rows(limit=prelimit)
    scored: List[Tuple[float, Dict[str, Any]]] = []
    for r in rows:
        try:
            vec = np.array(json.loads(r["profile_vector"]), dtype=np.float32)
            n = np.linalg.norm(vec)
            if n: vec = vec / n
            scored.append((cos_sim(qv, vec), r))
        except Exception:
            continue
    scored.sort(key=lambda x: x[0], reverse=True)
    return [row for _, row in scored[:k]]

def birthyear_to_age(birth: str | int) -> int | None:
    # ... (ê¸°ì¡´ birthyear_to_age í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    try:
        y = int(str(birth))
        return date.today().year - y
    except Exception:
        return None

# âœ¨ RAG Prompt Template (ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •)
def build_prompt(question: str, ctx_rows: List[Dict[str, Any]], history: List[Dict[str, str]]) -> str:
    """
    [ëŒ€í™”í˜• Prompt] ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” ê¸°ë¡ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    if not ctx_rows:
        ctx = "ì—†ìŒ"
    else:
        lines = []
        for r in ctx_rows:
            # í¬ë§·íŒ… ë¡œì§ ìœ ì§€
            sex = "ë‚¨ì„±" if r.get("Q10") == "M" else ("ì—¬ì„±" if r.get("Q10") == "F" else str(r.get("Q10")))
            age = birthyear_to_age(r.get("Q11"))
            loc = (r.get("Q12_1") or "")
            if r.get("Q12_2"): loc += f" {r.get('Q12_2')}"
            lines.append(f"- ì„±ë³„={sex} | ë‚˜ì´={(str(age)+'ì„¸') if age is not None else 'ì •ë³´ì—†ìŒ'} | ì§€ì—­={loc}")
        ctx = "\n".join(lines)
    
    # âœ¨ ëŒ€í™” ê¸°ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    history_str = "\n".join([f"{h['role'].capitalize()}: {h['content']}" for h in history])

    return f"""ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ì™€ 'ì´ì „ ëŒ€í™”'ë¥¼ ì°¸ê³ í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ 'í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì‹­ì‹œì˜¤.
IDë‚˜ ì¶œì²˜ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤.

[ì´ì „ ëŒ€í™”]
{history_str}

[ì§ˆë¬¸]
{question}

[ì»¨í…ìŠ¤íŠ¸]
{ctx}
"""

# =================================================================
# 3) OpenAI (íˆ´ì½œ ë° RAG ë¡œì§)
# =================================================================

client = OpenAI(api_key=OPENAI_API_KEY)

# Tool Call ê´€ë ¨ SYSTEM PROMPT ë° TOOLS ì •ì˜ëŠ” ê¸°ì¡´ íŒŒì¼ ë¡œì§ì„ ìœ ì§€

SYSTEM = (
    "ë„ˆëŠ” ì—…ë¡œë“œëœ ê³ ê° í…Œì´ë¸”ë§Œìœ¼ë¡œ ë‹µí•œë‹¤. "
    "í•„ìš”í•˜ë©´ ì•„ë˜ ë„êµ¬ë¥¼ 0~1íšŒ í˜¸ì¶œí•´ ì •í™•í•œ ìˆ«ì/1ìœ„ ì§€ì—­ì„ ì–»ê³ , "
    "ê·¸ ì™¸ ì„œìˆ í˜•ì€ ë‚´ê°€ ì¤€ ì»¨í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µí•˜ë¼. "
    "ID/ì¶œì²˜ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ê³ , ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ë‹µí•´ë¼."
)

TOOLS = [
    # ... (ê¸°ì¡´ TOOLS ì •ì˜ ìœ ì§€) ...
    {
        "type": "function",
        "function": {
            "name": "count_people",
            "description": "ì¡°ê±´ì— ë§ëŠ” ì¸ì› ìˆ˜ë¥¼ DBì—ì„œ ì§‘ê³„í•œë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "gender":   {"type": ["string","null"], "description": "ì„±ë³„: 'ë‚¨' ë˜ëŠ” 'ì—¬' ë˜ëŠ” 'M'/'F'"},
                    "decade":   {"type": ["integer","null"], "description": "ì—°ë ¹ëŒ€(10ì˜ ìë¦¬): 20,30,40 ..."},
                    "sido":     {"type": ["string","null"], "description": "ì‹œ/ë„(ì„œìš¸, ê²½ê¸° ë“±)"},
                    "sigungu": {"type": ["string","null"], "description": "ì‹œ/êµ°/êµ¬(ì„±ë™êµ¬, ë¶„ë‹¹êµ¬ ë“±)"},
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "most_region",
            "description": "ì¡°ê±´ì— ë§ëŠ” ì§€ì—­ ë ˆë²¨ì—ì„œ 'ê°€ì¥ ë§ì€' 1ê°œ ì§€ì—­ë§Œ ë°˜í™˜í•œë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "gender":   {"type": ["string","null"]},
                    "decade":   {"type": ["integer","null"]},
                    "sido":     {"type": ["string","null"]},
                    "sigungu": {"type": ["string","null"]},
                    "level":   {"type": "string", "enum": ["sido", "sigungu"], "description": "ì§‘ê³„ ë ˆë²¨"},
                },
                "required": ["level"],
            },
        },
    },
]

# Tool Call ì§€ì› í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€
def _normalize_gender(g: str | None) -> str | None:
    # ... (ê¸°ì¡´ _normalize_gender í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    if not g: return None
    if g in ("ë‚¨","ë‚¨ì„±","M","m"): return "M"
    if g in ("ì—¬","ì—¬ì„±","F","f"): return "F"
    return None

def tool_count_people(args: Dict[str, Any]) -> Dict[str, Any]:
    # ... (ê¸°ì¡´ tool_count_people í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    flt = {
        "gender": _normalize_gender(args.get("gender")),
        "decade": args.get("decade"),
        "sido": args.get("sido"),
        "sigungu": args.get("sigungu"),
    }
    where_sql, params = _make_where_and_params(flt)
    sql = f"SELECT COUNT(*) FROM respondents {where_sql}"
    conn = get_conn()
    if not conn: return {"ok": False, "error": "DB connection failed"}
    with conn.cursor() as cur:
        cur.execute(sql, params)
        cnt = cur.fetchone()[0]
    return {"ok": True, "count": int(cnt), "filters": flt}

def tool_most_region(args: Dict[str, Any]) -> Dict[str, Any]:
    # ... (ê¸°ì¡´ tool_most_region í•¨ìˆ˜ ë¡œì§ ìœ ì§€) ...
    flt = {
        "gender": _normalize_gender(args.get("gender")),
        "decade": args.get("decade"),
        "sido": args.get("sido"),
        "sigungu": args.get("sigungu"),
    }
    level = args.get("level","sido")
    group_col = '"Q12_2"' if level == "sigungu" else '"Q12_1"'
    where_sql, params = _make_where_and_params(flt)
    sql = f'''
     SELECT {group_col} AS g, COUNT(*) AS c
     FROM respondents
     {where_sql}
     GROUP BY {group_col}
     ORDER BY c DESC
     LIMIT 1
    '''
    conn = get_conn()
    if not conn: return {"ok": False, "error": "DB connection failed"}
    with conn.cursor() as cur:
        cur.execute(sql, params)
        row = cur.fetchone()
    if not row or not row[0]: return {"ok": True, "region": None, "count": 0, "level": level, "filters": flt}
    return {"ok": True, "region": row[0], "count": int(row[1]), "level": level, "filters": flt}


# âœ¨ llm_answer í•¨ìˆ˜ ìˆ˜ì • (History ì ìš© ë° ê°„í¸/ëŒ€í™”í˜• ë¶„ê¸°)
def llm_answer(question: str, session_id: str | None = None, mode: str = "conv") -> str:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ RAG/Tool Call ë° ëŒ€í™” ê¸°ë¡ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
    """
    
    # 1. History ë¡œë“œ (ëŒ€í™”í˜• ëª¨ë“œì¼ ê²½ìš°)
    history = CONVERSATION_HISTORY.get(session_id, []) if mode == "conv" and session_id else []
    
    # 2. íˆ´ì½œ ì‹œë„ (ì£¼ë¡œ ìˆ«ìë‚˜ ì§€ì—­ 1ìœ„ ìš”ì²­ ì‹œ LLMì´ í˜¸ì¶œì„ ê²°ì •)
    messages = [{"role": "system", "content": SYSTEM}] + history + [{"role": "user", "content": question}]
    
    first = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=messages,
        tools=TOOLS,
        tool_choice="auto",
    )
    m = first.choices[0].message

    # 3. íˆ´ì½œ ì²˜ë¦¬ (ìˆì„ ê²½ìš°)
    if m.tool_calls:
        tool_outputs_msgs = []
        for tc in m.tool_calls:
            name = tc.function.name
            args = json.loads(tc.function.arguments or "{}")
            
            # Tool Call ì§€ì› í•¨ìˆ˜ ë¶„ê¸°
            if name == "count_people":
                out = tool_count_people(args)
            elif name == "most_region":
                out = tool_most_region(args)
            else:
                out = {"ok": False, "error": f"unknown tool {name}"}

            tool_outputs_msgs.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": name,
                "content": json.dumps(out, ensure_ascii=False),
            })

        # ë„êµ¬ ê²°ê³¼ë¥¼ ë°˜ì˜í•´ ìµœì¢… ë‹µë³€ ìƒì„± (Follow-up Call)
        messages.append({"role": "assistant", "tool_calls": m.tool_calls, "content": ""})
        messages.extend(tool_outputs_msgs)
        
        follow = client.chat.completions.create(
            model=CHAT_MODEL,
            temperature=0.2,
            messages=messages,
        )
        final_content = follow.choices[0].message.content.strip()
        
    # 4. íˆ´ì½œì´ ì—†ìœ¼ë©´ â†’ RAG ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€
    else:
        # RAG ê²€ìƒ‰ ë° Prompt ìƒì„±
        ctx = fetch_topk_by_cosine(question, k=5)
        # Historyì™€ Contextë¥¼ í¬í•¨í•˜ì—¬ Prompt ìƒì„±
        prompt = build_prompt(question, ctx, history) 
        
        final = client.chat.completions.create(
            model=CHAT_MODEL,
            temperature=0.2,
            messages=[
                {"role": "system", "content": SYSTEM},
                {"role": "user", "content": prompt},
            ],
        )
        final_content = final.choices[0].message.content.strip()

    # 5. History ì €ì¥ (ëŒ€í™”í˜• ëª¨ë“œì¼ ê²½ìš°)
    if mode == "conv" and session_id:
        # User ì§ˆë¬¸ê³¼ AI ë‹µë³€ì„ Historyì— ì¶”ê°€
        history_entry = [
            {"role": "user", "content": question},
            {"role": "assistant", "content": final_content}
        ]
        if session_id not in CONVERSATION_HISTORY:
            CONVERSATION_HISTORY[session_id] = []
        
        CONVERSATION_HISTORY[session_id].extend(history_entry)
        
        # Historyê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ê´€ë¦¬ (ì„ íƒ ì‚¬í•­)
        MAX_HISTORY_LENGTH = 6 # ìµœëŒ€ 3ìŒì˜ ëŒ€í™”ë§Œ ìœ ì§€
        if len(CONVERSATION_HISTORY[session_id]) > MAX_HISTORY_LENGTH:
            CONVERSATION_HISTORY[session_id] = CONVERSATION_HISTORY[session_id][-MAX_HISTORY_LENGTH:]


    return final_content

# =================================================================
# 4) API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • (Session ID ë° Mode ë°›ê¸°)
# =================================================================

@app.route("/api/chat-search", methods=["POST"])
def chat_search():
    data = request.get_json() or {}
    q = (data.get("query") or "").strip()
    session_id = data.get("session_id") # âœ¨ Session ID ì¶”ê°€
    mode = data.get("mode", "conv")    # âœ¨ Mode (conv/simple) ì¶”ê°€
    
    if not q:
        return jsonify({"error": "Query is required"}), 400
        
    if not session_id:
        session_id = str(uuid4()) # IDê°€ ì—†ìœ¼ë©´ ì„ì‹œë¡œ ìƒì„±

    if any(w in q for w in OOS_WORDS):
        return jsonify({
            "id": f"ai-{uuid4().hex}",
            "type": "ai",
            "role": "assistant",
            "content": "ì´ ì„œë¹„ìŠ¤ëŠ” ì—…ë¡œë“œëœ ë°ì´í„°(í…Œì´ë¸”)ì— ëŒ€í•œ ì§ˆë¬¸ë§Œ ë‹µë³€í•©ë‹ˆë‹¤.",
            "session_id": session_id
        })

    # âœ¨ llm_answer í˜¸ì¶œ ì‹œ session_idì™€ mode ì „ë‹¬
    content = llm_answer(q, session_id=session_id, mode=mode) 
    
    return jsonify({
        "id": f"ai-{uuid4().hex}",
        "type": "ai",
        "role": "assistant",
        "content": content,
        "session_id": session_id # ì‘ë‹µì—ë„ session_id í¬í•¨
    })


# =================================================================
# 5) main
# =================================================================
if __name__ == "__main__":
    print(f"ğŸ”‘ OPENAI_API_KEY loaded? {bool(OPENAI_API_KEY)} | .env: {ENV_PATH}")
    print(f"ğŸ§  EMBED_MODEL: {EMBED_MODEL} | ğŸ’¬ CHAT_MODEL: {CHAT_MODEL}")
    app.run(host="0.0.0.0", port=5000, debug=True)