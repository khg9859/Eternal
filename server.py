# -*- coding: utf-8 -*-
import os, re, json
from uuid import uuid4
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Any, Tuple

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv

from sentence_transformers import SentenceTransformer
from openai import OpenAI

# =========================
# 0) í™˜ê²½
# =========================
BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR / ".env"
load_dotenv(ENV_PATH)

DB = dict(
    host=os.getenv("DB_HOST", "localhost"),
    port=int(os.getenv("DB_PORT", "5432")),
    dbname=os.getenv("DB_NAME", "mydb"),
    user=os.getenv("DB_USER", "postgres"),
    password=os.getenv("DB_PASSWORD", "7302"),
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_MODEL     = os.getenv("CHAT_MODEL", "gpt-4o-mini")
EMBED_MODEL    = os.getenv("EMBED_MODEL", "nlpai-lab/KURE-v1")  # KURE ìš°ì„ , ì‹¤íŒ¨ ì‹œ í´ë°±

OOS_WORDS = ["ë‚ ì”¨","ì£¼ê°€","í™˜ìœ¨","ë‰´ìŠ¤","êµí†µ","ì‹œê°„","ì£¼ì†Œ","íƒë°°"]

app = Flask(__name__)
CORS(app)

# =========================
# 1) DB
# =========================
def get_conn():
    try:
        return psycopg2.connect(**DB)
    except Exception as e:
        print(f"[DB] connection failed: {e}")
        return None

# í˜„ ìŠ¤í‚¤ë§ˆ ê°€ì •:
# respondents(mb_sn, Q10(ì„±ë³„:M/F), Q11(ì¶œìƒë…„ë„:int), Q12_1(ì‹œ/ë„), Q12_2(ì‹œêµ°êµ¬), profile_vector(pgvector))
def fetch_rows(limit: int | None = None) -> List[Dict[str, Any]]:
    conn = get_conn()
    if not conn:
        return []
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            sql = 'SELECT mb_sn, "Q10", "Q11", "Q12_1", "Q12_2", profile_vector::text AS profile_vector FROM respondents'
            if limit:
                sql += " LIMIT %s"
                cur.execute(sql, (limit,))
            else:
                cur.execute(sql)
            return cur.fetchall()
    finally:
        conn.close()

# whereì ˆ ìƒì„± (ì„±ë³„/ì—°ë ¹ëŒ€/ì‹œë„/ì‹œêµ°êµ¬)
def _make_where_and_params(filters: Dict[str, Any]) -> tuple[str, list]:
    where, params = [], []

    # ì„±ë³„: 'ë‚¨'/'ì—¬' ë˜ëŠ” 'M'/'F' ëª¨ë‘ í—ˆìš©
    g = filters.get("gender")
    if g:
        if g in ("ë‚¨","ì—¬"):
            g = "M" if g == "ë‚¨" else "F"
        where.append('"Q10" = %s')
        params.append(g)

    # ì—°ë ¹ëŒ€: decade=30 â†’ 30~39ì„¸
    dec = filters.get("decade")
    if dec is not None:
        lo, hi = int(dec), int(dec)+9
        where.append('(EXTRACT(YEAR FROM CURRENT_DATE)::int - "Q11"::int BETWEEN %s AND %s)')
        params += [lo, hi]

    # ì‹œ/ë„
    if filters.get("sido"):
        where.append('"Q12_1" = %s')
        params.append(filters["sido"])

    # ì‹œêµ°êµ¬
    if filters.get("sigungu"):
        where.append('"Q12_2" = %s')
        params.append(filters["sigungu"])

    return (("WHERE " + " AND ".join(where)) if where else ""), params

# =========================
# 2) ì„ë² ë”© (KURE â†’ í´ë°±)
# =========================
@lru_cache(maxsize=1)
def _load_embedder():
    print(f"[EMBED] loading: {EMBED_MODEL}")
    try:
        return SentenceTransformer(EMBED_MODEL)
    except Exception as e:
        print(f"[EMBED] primary load failed: {e}")
        fb = "jhgan/ko-sroberta-multitask"
        print(f"[EMBED] fallback -> {fb}")
        return SentenceTransformer(fb)

@lru_cache(maxsize=4096)
def embed_text(text: str) -> np.ndarray:
    model = _load_embedder()
    v = np.array(model.encode(text, normalize_embeddings=True), dtype=np.float32)
    n = np.linalg.norm(v)
    return (v / n) if n else v

def cos_sim(u: np.ndarray, v: np.ndarray) -> float:
    return float(np.dot(u, v))

def fetch_topk_by_cosine(query: str, k: int = 5, prelimit: int = 800) -> List[Dict[str, Any]]:
    qv = embed_text(query)
    rows = fetch_rows(limit=prelimit)
    scored: List[Tuple[float, Dict[str, Any]]] = []
    for r in rows:
        try:
            vec = np.array(json.loads(r["profile_vector"]), dtype=np.float32)
            n = np.linalg.norm(vec)
            if n: vec = vec / n
            scored.append((cos_sim(qv, vec), r))
        except Exception:
            continue
    scored.sort(key=lambda x: x[0], reverse=True)
    return [row for _, row in scored[:k]]

def birthyear_to_age(birth: str | int) -> int | None:
    try:
        y = int(str(birth))
        from datetime import date
        return date.today().year - y
    except Exception:
        return None

def build_prompt(question: str, ctx_rows: List[Dict[str, Any]]) -> str:
    if not ctx_rows:
        ctx = "ì—†ìŒ"
    else:
        lines = []
        for r in ctx_rows:
            sex = "ë‚¨ì„±" if r.get("Q10") == "M" else ("ì—¬ì„±" if r.get("Q10") == "F" else str(r.get("Q10")))
            age = birthyear_to_age(r.get("Q11"))
            loc = (r.get("Q12_1") or "")
            if r.get("Q12_2"): loc += f" {r.get('Q12_2')}"
            lines.append(f"- ì„±ë³„={sex} | ë‚˜ì´={(str(age)+'ì„¸') if age is not None else 'ì •ë³´ì—†ìŒ'} | ì§€ì—­={loc}")
        ctx = "\n".join(lines)

    return f"""ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ë§Œ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ 'í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì‹­ì‹œì˜¤.
IDë‚˜ ì¶œì²˜ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤.

[ì§ˆë¬¸]
{question}

[ì»¨í…ìŠ¤íŠ¸]
{ctx}
"""

# =========================
# 3) OpenAI (íˆ´ì½œ)
# =========================
client = OpenAI(api_key=OPENAI_API_KEY)

SYSTEM = (
    "ë„ˆëŠ” ì—…ë¡œë“œëœ ê³ ê° í…Œì´ë¸”ë§Œìœ¼ë¡œ ë‹µí•œë‹¤. "
    "í•„ìš”í•˜ë©´ ì•„ë˜ ë„êµ¬ë¥¼ 0~1íšŒ í˜¸ì¶œí•´ ì •í™•í•œ ìˆ«ì/1ìœ„ ì§€ì—­ì„ ì–»ê³ , "
    "ê·¸ ì™¸ ì„œìˆ í˜•ì€ ë‚´ê°€ ì¤€ ì»¨í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µí•˜ë¼. "
    "ID/ì¶œì²˜ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ê³ , ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ë‹µí•´ë¼."
)

# tools ìŠ¤í‚¤ë§ˆ
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "count_people",
            "description": "ì¡°ê±´ì— ë§ëŠ” ì¸ì› ìˆ˜ë¥¼ DBì—ì„œ ì§‘ê³„í•œë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "gender":  {"type": ["string","null"], "description": "ì„±ë³„: 'ë‚¨' ë˜ëŠ” 'ì—¬' ë˜ëŠ” 'M'/'F'"},
                    "decade":  {"type": ["integer","null"], "description": "ì—°ë ¹ëŒ€(10ì˜ ìë¦¬): 20,30,40 ..."},
                    "sido":    {"type": ["string","null"], "description": "ì‹œ/ë„(ì„œìš¸, ê²½ê¸° ë“±)"},
                    "sigungu": {"type": ["string","null"], "description": "ì‹œ/êµ°/êµ¬(ì„±ë™êµ¬, ë¶„ë‹¹êµ¬ ë“±)"},
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "most_region",
            "description": "ì¡°ê±´ì— ë§ëŠ” ì§€ì—­ ë ˆë²¨ì—ì„œ 'ê°€ì¥ ë§ì€' 1ê°œ ì§€ì—­ë§Œ ë°˜í™˜í•œë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "gender":  {"type": ["string","null"]},
                    "decade":  {"type": ["integer","null"]},
                    "sido":    {"type": ["string","null"]},
                    "sigungu": {"type": ["string","null"]},
                    "level":   {"type": "string", "enum": ["sido", "sigungu"], "description": "ì§‘ê³„ ë ˆë²¨"},
                },
                "required": ["level"],
            },
        },
    },
]

def _normalize_gender(g: str | None) -> str | None:
    if not g: return None
    if g in ("ë‚¨","ë‚¨ì„±","M","m"): return "M"
    if g in ("ì—¬","ì—¬ì„±","F","f"): return "F"
    return None

def tool_count_people(args: Dict[str, Any]) -> Dict[str, Any]:
    flt = {
        "gender": _normalize_gender(args.get("gender")),
        "decade": args.get("decade"),
        "sido": args.get("sido"),
        "sigungu": args.get("sigungu"),
    }
    where_sql, params = _make_where_and_params(flt)
    sql = f"SELECT COUNT(*) FROM respondents {where_sql}"
    conn = get_conn()
    if not conn:
        return {"ok": False, "error": "DB connection failed"}
    with conn.cursor() as cur:
        cur.execute(sql, params)
        cnt = cur.fetchone()[0]
    return {"ok": True, "count": int(cnt), "filters": flt}

def tool_most_region(args: Dict[str, Any]) -> Dict[str, Any]:
    flt = {
        "gender": _normalize_gender(args.get("gender")),
        "decade": args.get("decade"),
        "sido": args.get("sido"),
        "sigungu": args.get("sigungu"),
    }
    level = args.get("level","sido")
    group_col = '"Q12_2"' if level == "sigungu" else '"Q12_1"'
    where_sql, params = _make_where_and_params(flt)
    sql = f'''
      SELECT {group_col} AS g, COUNT(*) AS c
      FROM respondents
      {where_sql}
      GROUP BY {group_col}
      ORDER BY c DESC
      LIMIT 1
    '''
    conn = get_conn()
    if not conn:
        return {"ok": False, "error": "DB connection failed"}
    with conn.cursor() as cur:
        cur.execute(sql, params)
        row = cur.fetchone()
    if not row or not row[0]:
        return {"ok": True, "region": None, "count": 0, "level": level, "filters": flt}
    return {"ok": True, "region": row[0], "count": int(row[1]), "level": level, "filters": flt}

def llm_answer(question: str) -> str:
    """
    1) ë¨¼ì € íˆ´ì½œì„ ì‹œë„ (count / most_region)
    2) íˆ´ì½œì´ ì—†ê±°ë‚˜ ë¶ˆí•„ìš”í•˜ë©´ RAGë¡œ ë¬¸ì¥ ìƒì„±
    """
    # 1) íˆ´ì½œ ì‹œë„
    msg = [
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": question},
    ]
    first = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=msg,
        tools=TOOLS,
        tool_choice="auto",
    )
    m = first.choices[0].message

    # íˆ´ì½œ ì²˜ë¦¬
    if m.tool_calls:
        # ìµœëŒ€ í•œ ë²ˆë§Œ ìˆ˜í–‰(ì¼ë°˜ì ìœ¼ë¡œ 0~1íšŒë©´ ì¶©ë¶„)
        tool_outputs_msgs = []
        for tc in m.tool_calls:
            name = tc.function.name
            args = json.loads(tc.function.arguments or "{}")
            if name == "count_people":
                out = tool_count_people(args)
            elif name == "most_region":
                out = tool_most_region(args)
            else:
                out = {"ok": False, "error": f"unknown tool {name}"}

            tool_outputs_msgs.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": name,
                "content": json.dumps(out, ensure_ascii=False),
            })

        # ë„êµ¬ ê²°ê³¼ë¥¼ ë°˜ì˜í•´ ìµœì¢… ë‹µë³€ ìƒì„±
        follow = client.chat.completions.create(
            model=CHAT_MODEL,
            temperature=0.2,
            messages=msg + [
                {"role": "assistant", "tool_calls": m.tool_calls, "content": ""},
                *tool_outputs_msgs,
            ],
        )
        return follow.choices[0].message.content.strip()

    # 2) íˆ´ì½œì´ ì—†ìœ¼ë©´ â†’ RAG ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€
    ctx = fetch_topk_by_cosine(question, k=5)
    prompt = build_prompt(question, ctx)
    final = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=[
            {"role": "system", "content": SYSTEM},
            {"role": "user", "content": prompt},
        ],
    )
    return final.choices[0].message.content.strip()

# =========================
# 4) API
# =========================
@app.route("/api/chat-search", methods=["POST"])
def chat_search():
    data = request.get_json() or {}
    q = (data.get("query") or "").strip()
    if not q:
        return jsonify({"error": "Query is required"}), 400

    if any(w in q for w in OOS_WORDS):
        return jsonify({
            "id": f"ai-{uuid4().hex}",
            "type": "ai",
            "role": "assistant",
            "content": "ì´ ì„œë¹„ìŠ¤ëŠ” ì—…ë¡œë“œëœ ë°ì´í„°(í…Œì´ë¸”)ì— ëŒ€í•œ ì§ˆë¬¸ë§Œ ë‹µë³€í•©ë‹ˆë‹¤."
        })

    content = llm_answer(q)
    return jsonify({
        "id": f"ai-{uuid4().hex}",
        "type": "ai",
        "role": "assistant",
        "content": content
    })

# =========================
# 5) main
# =========================
if __name__ == "__main__":
    print(f"ğŸ”‘ OPENAI_API_KEY loaded? {bool(OPENAI_API_KEY)} | .env: {ENV_PATH}")
    print(f"ğŸ§  EMBED_MODEL: {EMBED_MODEL} | ğŸ’¬ CHAT_MODEL: {CHAT_MODEL}")
    app.run(host="0.0.0.0", port=5000, debug=True)

