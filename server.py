# -*- coding: utf-8 -*-
import os, re, json
from uuid import uuid4
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor

from dotenv import load_dotenv

from sentence_transformers import SentenceTransformer
from openai import OpenAI

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# =========================
# 0) í™˜ê²½
# =========================
BASE_DIR = Path(__file__).resolve().parent
ENV_PATH = BASE_DIR / ".env"
load_dotenv(ENV_PATH)

DB = dict(
    host=os.getenv("DB_HOST", "localhost"),
    port=int(os.getenv("DB_PORT", "5432")),
    dbname=os.getenv("DB_NAME", "survey_db"),
    user=os.getenv("DB_USER", "postgres"),
    password=os.getenv("DB_PASSWORD", "7302"),
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_MODEL     = os.getenv("CHAT_MODEL", "gpt-4o-mini")
EMBED_MODEL    = os.getenv("EMBED_MODEL", "nlpai-lab/KURE-v1")

OOS_WORDS = ["ë‚ ì”¨","ì£¼ê°€","í™˜ìœ¨","ë‰´ìŠ¤","êµí†µ","ì‹œê°„","ì£¼ì†Œ","íƒë°°"]

# ğŸ” FastAPI ì•± & CORS
app = FastAPI(title="Chat Search API (FastAPI)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# 1) DB / ê³µí†µ
# =========================
def get_conn():
    try:
        return psycopg2.connect(**DB)
    except Exception as e:
        print(f"[DB] connection failed: {e}")
        return None

# "ë§Œ NN ì„¸" ê°™ì€ age í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ
AGE_NUM_RE = re.compile(r"ë§Œ\s*(\d{1,3})\s*ì„¸")

def parse_age_from_text(txt: Optional[str]) -> Optional[int]:
    if not txt:
        return None
    m = AGE_NUM_RE.search(txt)
    if m:
        try:
            return int(m.group(1))
        except Exception:
            return None
    # ì˜ˆì™¸: "nan" ë“±
    try:
        # í˜¹ì‹œ "NN ì„¸"ë§Œ ìˆì„ ìˆ˜ë„ ìˆìŒ
        n = int(re.findall(r"\d{1,3}", txt)[-1])
        return n
    except Exception:
        return None

def normalize_gender_kor(g: Optional[str]) -> Optional[str]:
    if not g:
        return None
    g = g.strip().lower()
    if g in ("m", "male", "ë‚¨", "ë‚¨ì„±"):
        return "ë‚¨ì„±"
    if g in ("f", "female", "ì—¬", "ì—¬ì„±"):
        return "ì—¬ì„±"
    return None

# =========================
# 2) ì»¨í…ìŠ¤íŠ¸(Top-K) - respondents.profile_vector + metadata ì¡°ì¸
# =========================
def fetch_rows_for_rag(prelimit: int = 800) -> List[Dict[str, Any]]:
    """
    respondents.profile_vectorë¥¼ ì½ê³ , metadataì—ì„œ ì„±ë³„/ë‚˜ì´í…ìŠ¤íŠ¸/ì§€ì—­ì„ ê°€ì ¸ì˜¨ë‹¤.
    """
    conn = get_conn()
    if not conn:
        return []
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            sql = """
            SELECT r.mb_sn,
                   r.profile_vector::text AS profile_vector,
                   m.gender AS gender_text,
                   m.age    AS age_text,
                   m.region AS region_text
            FROM respondents r
            LEFT JOIN metadata m ON m.mb_sn = r.mb_sn
            LIMIT %s
            """
            cur.execute(sql, (prelimit,))
            return cur.fetchall()
    finally:
        conn.close()

@lru_cache(maxsize=1)
def _load_embedder():
    print(f"[EMBED] loading: {EMBED_MODEL}")
    try:
        return SentenceTransformer(EMBED_MODEL)
    except Exception as e:
        print(f"[EMBED] primary load failed: {e}")
        fb = "jhgan/ko-sroberta-multitask"
        print(f"[EMBED] fallback -> {fb}")
        return SentenceTransformer(fb)

@lru_cache(maxsize=4096)
def embed_text(text: str) -> np.ndarray:
    model = _load_embedder()
    v = np.array(model.encode(text, normalize_embeddings=True), dtype=np.float32)
    n = np.linalg.norm(v)
    return (v / n) if n else v

def cos_sim(u: np.ndarray, v: np.ndarray) -> float:
    return float(np.dot(u, v))

def fetch_topk_by_cosine(query: str, k: int = 5, prelimit: int = 800) -> List[Dict[str, Any]]:
    qv = embed_text(query)
    rows = fetch_rows_for_rag(prelimit=prelimit)
    scored: List[Tuple[float, Dict[str, Any]]] = []
    for r in rows:
        try:
            vec = np.array(json.loads(r["profile_vector"]), dtype=np.float32)
            n = np.linalg.norm(vec)
            if n:
                vec = vec / n
            scored.append((cos_sim(qv, vec), r))
        except Exception:
            continue
    scored.sort(key=lambda x: x[0], reverse=True)
    return [row for _, row in scored[:k]]

def build_prompt(question: str, ctx_rows: List[Dict[str, Any]]) -> str:
    if not ctx_rows:
        ctx = "ì—†ìŒ"
    else:
        lines = []
        for r in ctx_rows:
            sex = r.get("gender_text") or "ì •ë³´ì—†ìŒ"
            # age_textì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•´ ì˜ˆì˜ê²Œ í‘œì‹œ
            age_num = parse_age_from_text(r.get("age_text"))
            age_disp = f"{age_num}ì„¸" if age_num is not None else (r.get("age_text") or "ì •ë³´ì—†ìŒ")
            region = r.get("region_text") or "ì •ë³´ì—†ìŒ"
            lines.append(f"- ì„±ë³„={sex} | ë‚˜ì´={age_disp} | ì§€ì—­={region}")
        ctx = "\n".join(lines)

    return f"""ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ë§Œ ì‚¬ìš©í•´ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ 'í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì‹­ì‹œì˜¤.
IDë‚˜ ì¶œì²˜ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤.

[ì§ˆë¬¸]
{question}

[ì»¨í…ìŠ¤íŠ¸]
{ctx}
"""

# =========================
# 3) OpenAI & íˆ´ì½œ(ì§‘ê³„ëŠ” metadataì—ì„œ)
# =========================
client = OpenAI(api_key=OPENAI_API_KEY)

SYSTEM = (
    "ë„ˆëŠ” ì—…ë¡œë“œëœ ê³ ê° í…Œì´ë¸”ë§Œìœ¼ë¡œ ë‹µí•œë‹¤. "
    "í•„ìš”í•˜ë©´ ì•„ë˜ ë„êµ¬ë¥¼ 0~1íšŒ í˜¸ì¶œí•´ ì •í™•í•œ ìˆ«ì/1ìœ„ ì§€ì—­ì„ ì–»ê³ , "
    "ê·¸ ì™¸ ì„œìˆ í˜•ì€ ë‚´ê°€ ì¤€ ì»¨í…ìŠ¤íŠ¸ë¡œë§Œ ë‹µí•˜ë¼. "
    "ID/ì¶œì²˜ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ê³ , ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ë‹µí•´ë¼."
)

TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "count_people",
            "description": "ì¡°ê±´ì— ë§ëŠ” ì¸ì› ìˆ˜(metadata ê¸°ë°˜)ë¥¼ ì§‘ê³„í•œë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "gender":  {"type": ["string","null"], "description": "ì„±ë³„: ë‚¨/ì—¬/M/F"},
                    "decade":  {"type": ["integer","null"], "description": "ì—°ë ¹ëŒ€(10ì˜ ìë¦¬): 20,30,40 ..."},
                    "region":  {"type": ["string","null"], "description": "ì˜ˆ: ì„œìš¸, ê²½ê¸°, ë¶€ì‚° ..."},
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "most_region",
            "description": "ì¡°ê±´ì— ë§ëŠ” ì‚¬ëŒë“¤ ì¤‘ ê°€ì¥ ë§ì€ ì§€ì—­ 1ê°œ(metadata.region).",
            "parameters": {
                "type": "object",
                "properties": {
                    "gender": {"type": ["string","null"]},
                    "decade": {"type": ["integer","null"]},
                },
                "required": [],
            },
        },
    },
]

def _where_for_metadata(filters: Dict[str, Any]) -> tuple[str, list]:
    """metadata í…Œì´ë¸” ì „ìš© WHERE ìƒì„± (gender/decade/region)"""
    where, params = [], []

    g = normalize_gender_kor(filters.get("gender"))
    if g:
        where.append("m.gender = %s")
        params.append(g)

    dec = filters.get("decade")
    if dec is not None:
        lo, hi = int(dec), int(dec) + 9
        # age_textì—ì„œ 'ë§Œ NN ì„¸' íŒŒì‹± í›„ between
        where.append("""
          CASE
            WHEN m.age IS NULL THEN NULL
            ELSE
              (REGEXP_REPLACE(m.age, '.*ë§Œ\\s*(\\d{1,3})\\s*ì„¸.*', '\\1'))::int
          END BETWEEN %s AND %s
        """)
        params += [lo, hi]

    if filters.get("region"):
        where.append("m.region = %s")
        params.append(filters["region"])

    return (("WHERE " + " AND ".join([w for w in where if w.strip()])) if where else ""), params

def tool_count_people(args: Dict[str, Any]) -> Dict[str, Any]:
    flt = {
        "gender": args.get("gender"),
        "decade": args.get("decade"),
        "region": args.get("region"),
    }
    where_sql, params = _where_for_metadata(flt)
    sql = f"SELECT COUNT(*) FROM metadata m {where_sql}"
    conn = get_conn()
    if not conn:
        return {"ok": False, "error": "DB connection failed"}
    with conn.cursor() as cur:
        cur.execute(sql, params)
        cnt = cur.fetchone()[0]
    return {"ok": True, "count": int(cnt), "filters": flt}

def tool_most_region(args: Dict[str, Any]) -> Dict[str, Any]:
    flt = {
        "gender": args.get("gender"),
        "decade": args.get("decade"),
    }
    where_sql, params = _where_for_metadata(flt)
    sql = f"""
      SELECT m.region AS g, COUNT(*) AS c
      FROM metadata m
      {where_sql}
      GROUP BY m.region
      ORDER BY c DESC NULLS LAST
      LIMIT 1
    """
    conn = get_conn()
    if not conn:
        return {"ok": False, "error": "DB connection failed"}
    with conn.cursor() as cur:
        cur.execute(sql, params)
        row = cur.fetchone()
    if not row or not row[0]:
        return {"ok": True, "region": None, "count": 0, "filters": flt}
    return {"ok": True, "region": row[0], "count": int(row[1]), "filters": flt}

def llm_answer(question: str) -> str:
    """
    1) íˆ´ì½œ ì‹œë„ (count_people / most_region)
    2) ì—†ê±°ë‚˜ ë¶ˆí•„ìš”í•˜ë©´ RAG(top-K respondents + metadata ìš”ì•½)ë¡œ ë‹µë³€
    """
    msg = [
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": question},
    ]
    first = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=msg,
        tools=TOOLS,
        tool_choice="auto",
    )
    m = first.choices[0].message

    if m.tool_calls:
        tool_outputs_msgs = []
        for tc in m.tool_calls:
            name = tc.function.name
            args = json.loads(tc.function.arguments or "{}")
            if name == "count_people":
                out = tool_count_people(args)
            elif name == "most_region":
                out = tool_most_region(args)
            else:
                out = {"ok": False, "error": f"unknown tool {name}"}

            tool_outputs_msgs.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": name,
                "content": json.dumps(out, ensure_ascii=False),
            })

        follow = client.chat.completions.create(
            model=CHAT_MODEL,
            temperature=0.2,
            messages=msg + [
                {"role": "assistant", "tool_calls": m.tool_calls, "content": ""},
                *tool_outputs_msgs,
            ],
        )
        return follow.choices[0].message.content.strip()

    # RAG ê²½ë¡œ
    ctx = fetch_topk_by_cosine(question, k=5)
    prompt = build_prompt(question, ctx)
    final = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=[
            {"role": "system", "content": SYSTEM},
            {"role": "user", "content": prompt},
        ],
    )
    return final.choices[0].message.content.strip()

# =========================
# 4) API
# =========================
class ChatSearchRequest(BaseModel):
    query: str

class ChatSearchResponse(BaseModel):
    id: str
    type: str
    role: str
    content: str

@app.post("/api/chat-search", response_model=ChatSearchResponse)
def chat_search(req: ChatSearchRequest):
    q = (req.query or "").strip()
    if not q:
        raise HTTPException(status_code=400, detail="Query is required")

    if any(w in q for w in OOS_WORDS):
        return ChatSearchResponse(
            id=f"ai-{uuid4().hex}",
            type="ai",
            role="assistant",
            content="ì´ ì„œë¹„ìŠ¤ëŠ” ì—…ë¡œë“œëœ ë°ì´í„°(í…Œì´ë¸”)ì— ëŒ€í•œ ì§ˆë¬¸ë§Œ ë‹µë³€í•©ë‹ˆë‹¤."
        )

    content = llm_answer(q)
    return ChatSearchResponse(
        id=f"ai-{uuid4().hex}",
        type="ai",
        role="assistant",
        content=content
    )

# =========================
# 5) main
# =========================
if __name__ == "__main__":
    import uvicorn
    print(f"ğŸ”‘ OPENAI_API_KEY loaded? {bool(OPENAI_API_KEY)} | .env: {ENV_PATH}")
    print(f"ğŸ§  EMBED_MODEL: {EMBED_MODEL} | ğŸ’¬ CHAT_MODEL: {CHAT_MODEL}")
    uvicorn.run("fastapi_server:app", host="0.0.0.0", port=5000, reload=True)
